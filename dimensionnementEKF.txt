___________________________________________
________________ METHODE___________________
___________________________________________
Dimensionnement du vecteur d'état EKF

Pose = [position orientation]
position (3Dof) = [x y z] ∈ R^3

orientation (3Dof) -> 2 solutions :
	angle d'euler = [yaw pitch rool] ∈ R^3 -> intuitif, 3 valeurs indépendantes mais singularités
	quaternion = [q_w q_x q_y q_z] ∈ R^4 -> pas de singularité mais 4 valeurs dépendantes (peu intuitif) mais obligé d'y passer si utilisation de gyroscope 3d


On choisi le quaternion pour représenter l'orientation :
 Pose = [x y z q_w q_x q_y q_z] ∈ R^7

quels capteurs proprioceptifs :
- accéléromètre -> intégration -> vitesse linéaire -> integration -> Position
- gyroscope (vitesses angulaires) -> intégration -> orientation

quels capteur(s) exteroceptif(s) :
- 1 caméra qui regarde les leds IR sur le drone + traitement d'image pour récuperer les positions apparantes des leds.

Re :
les estimations se font dans le repère du robot, des gyroscopes et des accéléromètres supposés alignés.
Pour faire une estimation de la position, on doit passer par une estimation des vitesses linéaires : V_xyz = [v_x v_y v_z] ∈ R^3




Vecteurs EKF en fonctionnement (après calibration):
X = [P_xyz q V_xyz] = [x y z q_w q_x q_y q_z v_x v_y v_z] ∈ R^10
U = [g_xyz a_xyz] = [g_x g_y g_z a_x a_y a_z] ∈ R^6 -> entrées mesures proprioceptifs (accéléromètre et gyroscope)
Z = [L1_xy L2_xy L3_xy L4_xy] = [x_l1c y_l1c x_l2c y_l2c x_l3c y_l3c x_l4c y_l4c] ∈ R^6 -> entrées mesures extéroceptifs (position des repères dans l'image)

avec des paramètres fixés :
b_a : biais des accéléromètres
b_g : biais des gyroscopes
l1_d : position de la led 1 dans le repère du drone
P_c : position de la caméra dans la scène
q_c : orientation de la caméra
f : distance focale

fonction de transition d'état :

X_n = f(X, U):
	
	ac_xyz = a_xyz-b_a				// compensation des bias des accéléromètres
	ae_xyz = q.conj().rotate(q_imu.rotate(ac_xyz))	// rotation de a_xyz par q.conj() et q_imu donne l'accélération dans le repère de l'environement -> ae_xyz
	
	v_x = v_x+ae_x*dt
	v_y = v_y+ae_y*dt
	v_z = v_z+ae_z*dt				// integration des accélération linéaires

	x = x+v_x*dt
	y = y+v_y*dt
	z = z+z_y*dt					// integration des vitesses linéaires

	
	gc_xyz = g_xyz - b_g;				// compensation bais gyroscopes
	s = quaternion(0, gc_x, gc_y, gc_z)		// Quaternion pur contenant les mesures gyroscopiques
	dq = 0.5*(q*s)					// quaternion décrivant le taux de changement d'orientation
	q_n = q + dt*dq					// integration des vitesses angulaires donne la nouvelle orientation
	q_n.normalize()					// normalisation

	X_n = [x y z q_w q_x q_y q_z v_x v_y v_z]




fonction de mesure :
Z = H(X):
	
	//calcul des positions des repères
	L1 = P_xyz + q.rotate(l1_d)
	L2 = P_xyz + q.rotate(l2_d)
	L3 = P_xyz + q.rotate(l3_d)
	L4 = P_xyz + q.rotate(l4_d)

	//projection des leds ir dans le plan camera
	L1_c = q_c.conjugate.rotate(L1-P_c)			// position de la led1 dans le repère de la camera
	L1_i = L1_c * f / z_l1c				// projection dans l'image

	L2_c = q_c.conjugate.rotate(L2-P_c)			// position de la led1 dans le repère de la camera
	L2_i = L2_c * f / z_l2c				// projection dans l'image

	L3_c = q_c.conjugate.rotate(L3-P_c)			// position de la led1 dans le repère de la camera
	L3_i = L3_c * f / z_l3c				// projection dans l'image

	L4_c = q_c.conjugate.rotate(L4-P_c)			// position de la led1 dans le repère de la camera
	L4_i = L4_c * f / z_l4c				// projection dans l'image
	
	Z = [x_l1c y_l1c x_l2c y_l2c x_l3c y_l3c x_l4c y_l4c];
